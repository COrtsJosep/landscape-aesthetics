{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fbf7557-5258-4c8e-a224-96dc66f601a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27470bd8-bbf0-45eb-a20a-36a076d72f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30ee0ad3-af26-4423-8e08-a6faa2289f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File download successful, save path is: ../../models/resnet-152-torch-places365.caffemodel\n",
      "File download successful, save path is: ../../models/resnet-152-torch-places365.prototxt\n"
     ]
    }
   ],
   "source": [
    "url = \"http://netdissect.csail.mit.edu/dissect/zoo/resnet-152-torch-places365.caffemodel\"\n",
    "save_path = \"/path/to/your/directory/resnet-152-torch-places365.caffemodel\"\n",
    "\n",
    "def download_file(url, save_path):\n",
    "    if os.path.exists(save_path):\n",
    "        print(\"The file already exists, no need to download.\")\n",
    "    else:\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(save_path, \"wb\") as file:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    file.write(chunk)\n",
    "            print(\"File download successful, save path is:\", save_path)\n",
    "        else:\n",
    "            print(\"File download failed, status code:\", response.status_code)\n",
    "\n",
    "files_to_download = [\n",
    "    {\n",
    "        \"url\": \"http://netdissect.csail.mit.edu/dissect/zoo/resnet-152-torch-places365.caffemodel\",\n",
    "        \"save_path\": \"../../models/resnet-152-torch-places365.caffemodel\"\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"http://netdissect.csail.mit.edu/dissect/zoo/resnet-152-torch-places365.prototxt\",\n",
    "        \"save_path\": \"../../models/resnet-152-torch-places365.prototxt\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for file in files_to_download:\n",
    "    download_file(file[\"url\"], file[\"save_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "621c96cc-355b-4140-8e1d-32b4815a7bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|                                 | 0/101 [00:57<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 53\u001b[0m\n\u001b[1;32m     43\u001b[0m blob \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mblobFromImage(\n\u001b[1;32m     44\u001b[0m     image,\n\u001b[1;32m     45\u001b[0m     scalefactor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     crop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     52\u001b[0m net\u001b[38;5;241m.\u001b[39msetInput(blob)\n\u001b[0;32m---> 53\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m top5_indices \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margsort()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m:][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     56\u001b[0m top5_probabilities \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m][top5_indices]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the path for the model architecture and weight files.\n",
    "file_location_path = Path.cwd()\n",
    "prototxt_path = '../../models/resnet-152-torch-places365.prototxt'\n",
    "caffemodel_path = '../../models/resnet-152-torch-places365.caffemodel'\n",
    "project_base_path = file_location_path.parent.parent\n",
    "ns6_wiki_paths = project_base_path / 'data' / 'processed' / 'landscape_or_not'\n",
    "image_folder = Path('/home/ubuntu/landscape-aesthetics')\n",
    "\n",
    "results = []\n",
    "# Load Caffe model.\n",
    "net = cv2.dnn.readNetFromCaffe(prototxt_path, caffemodel_path)\n",
    "\n",
    "category_mapping = {}\n",
    "with open('../../data/external/Places365/categories_places365.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split()\n",
    "        category_name = parts[0][3:]\n",
    "        category_id = int(parts[1])\n",
    "        category_mapping[category_id] = category_name\n",
    "\n",
    "pattern = re.compile(r\"^ns6_clean_(\\d+)\\.csv$\") \n",
    "\n",
    "for file_name in tqdm(os.listdir(ns6_wiki_paths), desc=\"Processing files\"):\n",
    "    file_path = ns6_wiki_paths / file_name\n",
    "    if file_path.is_file() and pattern.match(file_name):\n",
    "        label_file_path = Path(file_name)\n",
    "        labeled_csv_name = label_file_path.with_suffix('.csv')\n",
    "        result = project_base_path / 'data' / 'processed' / 'landscape_type' / labeled_csv_name\n",
    "        result.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        data = pd.read_csv(file_path, usecols=['image_path', 'prediction'])\n",
    "        data = data[data['prediction'] == 1]\n",
    "        image_paths = data['image_path'].tolist()\n",
    "        \n",
    "        for img_path in image_paths:\n",
    "            image_path = image_folder / img_path\n",
    "            \n",
    "            try:\n",
    "                # Read and process the image\n",
    "                image = cv2.imread(str(image_path))\n",
    "                if image is None:\n",
    "                    print(f\"Warning: Unable to load image {img_path}, skipping...\")\n",
    "                    continue  # Skip if image is not loaded\n",
    "\n",
    "                # Resize the image\n",
    "                image = cv2.resize(image, (224, 224))\n",
    "\n",
    "                # Create a blob from the image\n",
    "                blob = cv2.dnn.blobFromImage(\n",
    "                    image,\n",
    "                    scalefactor=1.0,\n",
    "                    size=(224, 224),\n",
    "                    mean=(123.675, 116.28, 103.53),\n",
    "                    swapRB=True,\n",
    "                    crop=False\n",
    "                )\n",
    "\n",
    "                net.setInput(blob)\n",
    "                output = net.forward()\n",
    "\n",
    "                # Extract top 5 predictions\n",
    "                top5_indices = output[0].argsort()[-5:][::-1]\n",
    "                top5_probabilities = output[0][top5_indices]\n",
    "                top5_categories = [category_mapping[idx] for idx in top5_indices]\n",
    "\n",
    "                # Collect results for this image\n",
    "                row = [img_path]\n",
    "                for category, probability in zip(top5_categories, top5_probabilities):\n",
    "                    row.extend([category, probability])\n",
    "                results.append(row)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {img_path}: {e}\")\n",
    "                continue  # If any error occurs, skip this image and proceed to the next one\n",
    "\n",
    "        # Save results to CSV\n",
    "        columns = [\"filename\", \"prediction1\", \"probability1\", \"prediction2\", \"probability2\",\n",
    "                   \"prediction3\", \"probability3\", \"prediction4\", \"probability4\", \n",
    "                   \"prediction5\", \"probability5\"]\n",
    "\n",
    "        df = pd.DataFrame(results, columns=columns)\n",
    "        df.to_csv(result, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c358a2e-2343-464d-93f5-3b5b66d4714c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
