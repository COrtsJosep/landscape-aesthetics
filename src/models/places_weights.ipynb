{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fbf7557-5258-4c8e-a224-96dc66f601a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6210693b-d22f-4be2-bc44-986c9afe73b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# Download the required files.\n",
    "\n",
    "def download_file(url, save_path):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful.\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded {save_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fad19667-454d-4968-bb65-d57faba109ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_for_binary_classification(model_dir='../../data/external/Places365/model'):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    model_file = os.path.join(model_dir, 'wideresnet50_places365.pth.tar')\n",
    "    script_file = os.path.join(model_dir, 'wideresnet.py')\n",
    "\n",
    "    if not os.path.exists(model_file):\n",
    "        print(f\"{model_file} does not exist. Downloading...\")\n",
    "        model_url = 'http://places2.csail.mit.edu/models_places365/wideresnet50_places365.pth.tar'\n",
    "        download_file(model_url, model_file)\n",
    "\n",
    "    if not os.path.exists(script_file):\n",
    "        print(f\"{script_file} does not exist. Downloading...\")\n",
    "        script_url = 'https://raw.githubusercontent.com/csailvision/places365/master/wideresnet.py'\n",
    "        download_file(script_url, script_file)\n",
    "\n",
    "    sys.path.append(model_dir)\n",
    "\n",
    "    try:\n",
    "        import wideresnet\n",
    "        model = wideresnet.resnet18(num_classes=365)\n",
    "        checkpoint = torch.load(model_file, map_location=lambda storage, loc: storage, weights_only=True)\n",
    "        state_dict = {str.replace(k, 'module.', ''): v for k, v in checkpoint['state_dict'].items()}\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
    "\n",
    "        print(\"Model for binary classification loaded successfully.\")\n",
    "        return model\n",
    "    except ImportError as e:\n",
    "        print(f\"Error importing wideresnet module: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2462bab8-0190-4b31-bdc7-3a576e88d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandscapesDataset(Dataset):\n",
    "    def __init__(self, data_frame, root_dir, transform=None):\n",
    "        self.data_frame = data_frame\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.label = self.data_frame['landscape']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.root_dir, self.data_frame.iloc[idx, 0]) \n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')  \n",
    "        except (IOError, SyntaxError) as e:\n",
    "            print(f\"Error opening image at {image_path}, skipping to next.\")\n",
    "            return self.__getitem__((idx + 1) % len(self))  \n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        label = int(self.data_frame.iloc[idx, 1])\n",
    "        return image, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71459913-7eb9-41f1-a181-4e8201e80f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 4000\n",
      "Validation set size: 1000\n"
     ]
    }
   ],
   "source": [
    "data_path = '/home/ubuntu/landscape-aesthetics/data/processed/landscape_handmade/landscapes.csv'\n",
    "image_folder = Path('/home/ubuntu/landscape-aesthetics')\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "        # transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)\n",
    "    ])\n",
    "\n",
    "image_dataset = LandscapesDataset(data_frame=data,\n",
    "                              root_dir=image_folder,\n",
    "                              transform=data_transforms)\n",
    "\n",
    "train_size = int(0.8 * len(image_dataset))\n",
    "val_size = len(image_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(image_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "print(f\"Training set size: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation set size: {len(val_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3125c0f-305d-4a88-b7f0-c4ca454eccee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/external/Places365/model/wideresnet50_places365.pth.tar does not exist. Downloading...\n",
      "Failed to download http://places2.csail.mit.edu/models_places365/wideresnet50_places365.pth.tar: 404 Client Error: Not Found for url: http://places2.csail.mit.edu/models_places365/wideresnet50_places365.pth.tar\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/external/Places365/model/wideresnet50_places365.pth.tar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_for_binary_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m, in \u001b[0;36mload_model_for_binary_classification\u001b[0;34m(model_dir)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwideresnet\u001b[39;00m\n\u001b[1;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m wideresnet\u001b[38;5;241m.\u001b[39mresnet18(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m365\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mreplace(k, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m): v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n",
      "File \u001b[0;32m~/anaconda3/envs/geoproject/lib/python3.12/site-packages/torch/serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/geoproject/lib/python3.12/site-packages/torch/serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/geoproject/lib/python3.12/site-packages/torch/serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/external/Places365/model/wideresnet50_places365.pth.tar'"
     ]
    }
   ],
   "source": [
    "model = load_model_for_binary_classification()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "def train_places365_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=10):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        total_batches = len(train_loader)\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=total_batches, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_idx, (images, labels) in progress_bar:\n",
    "            images, labels = torch.stack(images), torch.stack(labels)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = torch.stack(images), torch.stack(labels)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        scheduler.step()\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "    return train_losses, val_losses, model\n",
    "\n",
    "train_losses, val_losses, model = train_places365_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=10)\n",
    "\n",
    "torch.save(model.state_dict(), 'places365_binary_model_resnet50.pth')\n",
    "\n",
    "def plot_training_curves(train_losses, val_losses):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.plot(epochs, train_losses, 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, val_losses, 'ro-', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_curves(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1539a764-c81e-47f2-8ce6-30a683f2d08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1.0, Count: 1566, Ratio: 61.41%\n",
      "Label: 0.0, Count: 984, Ratio: 38.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|████████████████████| 2550/2550 [14:35<00:00,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.80%\n",
      "Recall: 79.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(weights=None)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model.load_state_dict(torch.load('../../models/places365_binary_model.pth', map_location='cpu', weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "data_path = '/home/ubuntu/landscape-aesthetics/data/processed/landscape_handmade/landscapes.csv'\n",
    "image_folder = Path('/home/ubuntu/landscape-aesthetics')\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "        # transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)\n",
    "    ])\n",
    "\n",
    "\n",
    "# sampled_data = data.sample(n=100, random_state=123) \n",
    "image_paths = data.iloc[:, 0].tolist()\n",
    "true_labels = data.iloc[:, 1].tolist()\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(true_labels)\n",
    "\n",
    "total_samples = len(true_labels)\n",
    "label_ratios = {label: count / total_samples for label, count in label_counts.items()}\n",
    "\n",
    "for label, ratio in label_ratios.items():\n",
    "    print(f\"Label: {label}, Count: {label_counts[label]}, Ratio: {ratio * 100:.2f}%\")\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for img_path in tqdm(image_paths, desc=\"Processing images\"):\n",
    "\n",
    "    image_path = os.path.join(image_folder, img_path) \n",
    "    \n",
    "    img = data_transforms(Image.open(image_path).convert('RGB')) .unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(img)\n",
    "    \n",
    "    _, predicted = torch.max(output, 1)\n",
    "    predictions.append(predicted.item())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "recall = recall_score(true_labels, predictions, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'Recall: {recall * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d48af-f352-443f-ad4b-bf2bb0776a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d718f4-d603-4e49-ac61-ca39c07c778d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
