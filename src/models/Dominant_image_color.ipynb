{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "807f29bb-b6c5-4e08-8480-abe7cfbab60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   9%|█▎             | 18604/211856 [09:48<1:41:57, 31.59it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m I1, I2, I3 \u001b[38;5;241m=\u001b[39m convert_to_ohta(image_rgb)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Convert to LST color space\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m L, S_lst, T \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_lst\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Append the average values of each color space to the results list\u001b[39;00m\n\u001b[1;32m     59\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_path\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(image_path),\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH_mean\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(H), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS_mean\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(S), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV_mean\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(V),\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI1_mean\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(I1), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI2_mean\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(I2), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI3_mean\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(I3),\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL_mean\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(L), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS_lst_mean\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(S_lst), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT_mean\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(T)\n\u001b[1;32m     64\u001b[0m })\n",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m, in \u001b[0;36mconvert_to_lst\u001b[0;34m(image_rgb)\u001b[0m\n\u001b[1;32m     17\u001b[0m R, G, B \u001b[38;5;241m=\u001b[39m image_rgb[:,:,\u001b[38;5;241m0\u001b[39m], image_rgb[:,:,\u001b[38;5;241m1\u001b[39m], image_rgb[:,:,\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     18\u001b[0m L \u001b[38;5;241m=\u001b[39m (R \u001b[38;5;241m+\u001b[39m G \u001b[38;5;241m+\u001b[39m B) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3.0\u001b[39m  \u001b[38;5;66;03m# Brightness\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m S \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Saturation\u001b[39;00m\n\u001b[1;32m     20\u001b[0m T \u001b[38;5;241m=\u001b[39m (R \u001b[38;5;241m-\u001b[39m B)  \u001b[38;5;66;03m# Hue\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m L, S, T\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "# from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# # Define function: Convert to Ohta color space\n",
    "# def convert_to_ohta(image_rgb):\n",
    "#     R, G, B = image_rgb[:,:,0], image_rgb[:,:,1], image_rgb[:,:,2]\n",
    "#     I1 = (R + G + B) / 3.0  # Brightness component\n",
    "#     I2 = (R - B) / 2.0      # Red-blue difference\n",
    "#     I3 = (2 * G - R - B) / 4.0  # Green-purple difference\n",
    "#     return I1, I2, I3\n",
    "\n",
    "# # Define function: Convert to LST color space\n",
    "# def convert_to_lst(image_rgb):\n",
    "#     R, G, B = image_rgb[:,:,0], image_rgb[:,:,1], image_rgb[:,:,2]\n",
    "#     L = (R + G + B) / 3.0  # Brightness\n",
    "#     S = np.sqrt(((R - G)**2 + (R - B)**2 + (G - B)**2) / 3.0)  # Saturation\n",
    "#     T = (R - B)  # Hue\n",
    "#     return L, S, T\n",
    "\n",
    "# # Read the CSV file\n",
    "# data_path = '/home/ubuntu/landscape-aesthetics/data/external/scenicornot/scenicornot.metadata.csv'\n",
    "# image_folder = Path('/home/ubuntu/landscape-aesthetics/data/external/scenicornot')\n",
    "\n",
    "# data = pd.read_csv(data_path)\n",
    "\n",
    "# # List to store the final results\n",
    "# results = []\n",
    "\n",
    "# # Iterate over each row in the CSV file with tqdm progress bar\n",
    "# for index, row in tqdm(data.iterrows(), total=data.shape[0], desc=\"Processing images\"):\n",
    "#     # Read image path\n",
    "#     image_path = image_folder / row['filename']  # Assuming 'filename' is the column name containing image file names\n",
    "    \n",
    "#     try:\n",
    "#         # Read the image\n",
    "#         image = cv2.imread(str(image_path))\n",
    "        \n",
    "#         if image is None:\n",
    "#             print(f\"Image not found: {image_path}, skipping...\")\n",
    "#             continue\n",
    "        \n",
    "#         # Convert BGR to RGB (OpenCV reads in BGR format by default)\n",
    "#         image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "#         # Convert to HSV color space\n",
    "#         image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "#         H, S, V = image_hsv[:,:,0], image_hsv[:,:,1], image_hsv[:,:,2]\n",
    "        \n",
    "#         # Convert to Ohta color space\n",
    "#         I1, I2, I3 = convert_to_ohta(image_rgb)\n",
    "        \n",
    "#         # Convert to LST color space\n",
    "#         L, S_lst, T = convert_to_lst(image_rgb)\n",
    "        \n",
    "#         # Append the average values of each color space to the results list\n",
    "#         results.append({\n",
    "#             'image_path': str(image_path),\n",
    "#             'H_mean': np.mean(H), 'S_mean': np.mean(S), 'V_mean': np.mean(V),\n",
    "#             'I1_mean': np.mean(I1), 'I2_mean': np.mean(I2), 'I3_mean': np.mean(I3),\n",
    "#             'L_mean': np.mean(L), 'S_lst_mean': np.mean(S_lst), 'T_mean': np.mean(T)\n",
    "#         })\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing image {image_path}: {e}\")\n",
    "#         continue  # Skip this image and move to the next one\n",
    "\n",
    "# # Convert results to a DataFrame\n",
    "# output_df = pd.DataFrame(results)\n",
    "\n",
    "# # Save results to a new CSV file\n",
    "# output_df.to_csv('/home/ubuntu/landscape-aesthetics/data/external/scenicornot/scenicornot_color_feature.csv', index=False)\n",
    "\n",
    "# print(\"Color space extraction is complete, results saved in 'scenicornot_color_feature.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95be3a8e-0f35-4c8e-8100-3ef0fc63e3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:   0%|                | 212/211856 [00:44<12:17:57,  4.78it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m image_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image)\n\u001b[1;32m     42\u001b[0m image_np \u001b[38;5;241m=\u001b[39m image_np\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# Reshape image data to 2D array\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m \u001b[43mKMeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m dominant_color \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mcluster_centers_  \u001b[38;5;66;03m# Get top three dominant colors\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Add image path and dominant colors to results list\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/geoproject/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/geoproject/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:1519\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialization complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;66;03m# run a k-means once\u001b[39;00m\n\u001b[0;32m-> 1519\u001b[0m labels, inertia, centers, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43mkmeans_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenters_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_n_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;66;03m# determine if these results are the best so far\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;66;03m# we chose a new run if it has a better inertia and the clustering is\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;66;03m# different from the best so far (it's possible that the inertia is\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;66;03m# slightly better even if the clustering is the same with potentially\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m \u001b[38;5;66;03m# permuted labels, due to rounding errors)\u001b[39;00m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_inertia \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1535\u001b[0m     inertia \u001b[38;5;241m<\u001b[39m best_inertia\n\u001b[1;32m   1536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_same_clustering(labels, best_labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters)\n\u001b[1;32m   1537\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/geoproject/lib/python3.12/site-packages/sklearn/utils/parallel.py:162\u001b[0m, in \u001b[0;36m_threadpool_controller_decorator.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m controller \u001b[38;5;241m=\u001b[39m _get_threadpool_controller()\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m controller\u001b[38;5;241m.\u001b[39mlimit(limits\u001b[38;5;241m=\u001b[39mlimits, user_api\u001b[38;5;241m=\u001b[39muser_api):\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/geoproject/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:707\u001b[0m, in \u001b[0;36m_kmeans_single_lloyd\u001b[0;34m(X, sample_weight, centers_init, max_iter, verbose, tol, n_threads)\u001b[0m\n\u001b[1;32m    704\u001b[0m strict_convergence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[0;32m--> 707\u001b[0m     \u001b[43mlloyd_iter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcenters_new\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_in_clusters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcenter_shift\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    719\u001b[0m         inertia \u001b[38;5;241m=\u001b[39m _inertia(X, sample_weight, centers, labels, n_threads)\n",
      "File \u001b[0;32m_k_means_lloyd.pyx:160\u001b[0m, in \u001b[0;36msklearn.cluster._k_means_lloyd.lloyd_iter_chunked_dense\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_k_means_common.pyx:180\u001b[0m, in \u001b[0;36msklearn.cluster._k_means_common._relocate_empty_clusters_dense\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/geoproject/lib/python3.12/site-packages/numpy/core/multiarray.py:346\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(condition, x, y)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    inner(a, b, /)\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    341\u001b[0m \n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, b)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mwhere)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwhere\u001b[39m(condition, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    348\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m    where(condition, [x, y], /)\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m           [ 0,  3, -1]])\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (condition, x, y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Set paths\n",
    "data_path = '/home/ubuntu/landscape-aesthetics/data/external/scenicornot/scenicornot.metadata.csv'\n",
    "image_folder = Path('/home/ubuntu/landscape-aesthetics/data/external/scenicornot') \n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Define image transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)\n",
    "])\n",
    "\n",
    "# List to store image paths and dominant colors\n",
    "dominant_colors_data = []\n",
    "\n",
    "# Loop through each image with a progress bar\n",
    "for idx in tqdm(range(len(data)), desc=\"Processing Images\"):\n",
    "    img_name = data.iloc[idx]['filename']\n",
    "    image_path = image_folder / Path(img_name)\n",
    "    try:\n",
    "        # Read and process the image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if image is None:\n",
    "            print(f\"Warning: Unable to load image {image_path}, skipping...\")\n",
    "            continue  # Skip if the image cannot be loaded\n",
    "\n",
    "        # Apply transformations\n",
    "        img = data_transforms(image)\n",
    "        \n",
    "        # Extract dominant colors\n",
    "        image_np = np.array(image)\n",
    "        image_np = image_np.reshape(-1, 3)  # Reshape image data to 2D array\n",
    "        kmeans = KMeans(n_clusters=3, random_state=0).fit(image_np)\n",
    "        dominant_color = kmeans.cluster_centers_  # Get top three dominant colors\n",
    "        \n",
    "        # Add image path and dominant colors to results list\n",
    "        dominant_colors_data.append([str(image_path), *dominant_color.flatten()])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        continue  # Skip this image and proceed if an error occurs\n",
    "\n",
    "# Save results to CSV file\n",
    "output_df = pd.DataFrame(dominant_colors_data, columns=['image_path', 'R1', 'G1', 'B1', 'R2', 'G2', 'B2', 'R3', 'G3', 'B3'])\n",
    "output_df.to_csv('/home/ubuntu/landscape-aesthetics/reports/dominant_colors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dce23b0-b7eb-48d0-bd75-588ee289761e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
