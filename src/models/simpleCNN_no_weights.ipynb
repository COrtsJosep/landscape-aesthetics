{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f439431c-b4d8-4929-a24a-003ed58013ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e690e83-b2b7-429c-be33-2663914d37dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 1 files:\n",
      "/home/ubuntu/landscape-aesthetics/data/external/scenicornot/photos/76/41/764143_e860c8c4.jpg\n"
     ]
    }
   ],
   "source": [
    "# set up the path\n",
    "data_path = '/home/ubuntu/landscape-aesthetics/data/external/scenicornot/scenicornot.metadata.csv'\n",
    "image_folder = Path('/home/ubuntu/landscape-aesthetics/data/external/scenicornot') \n",
    "\n",
    "# Check if the data path exists\n",
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f\"Data file not found at {data_path}\")\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Check if the root directory exists\n",
    "root_dir = '/home/ubuntu/landscape-aesthetics/data/external/scenicornot/'\n",
    "if not os.path.exists(root_dir):\n",
    "    raise FileNotFoundError(f\"Root directory not found at {root_dir}\")\n",
    "\n",
    "# prepocess the dataset, remove the invalid paths\n",
    "valid_files = []\n",
    "missing_files = []\n",
    "for idx in range(len(data)):\n",
    "    img_name = data.iloc[idx]['filename']\n",
    "    image_path = image_folder / Path(img_name)\n",
    "    if image_path.exists():\n",
    "        valid_files.append(idx)\n",
    "    else:\n",
    "        missing_files.append(image_path)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"Missing {len(missing_files)} files:\")\n",
    "    for file in missing_files:\n",
    "        print(file)\n",
    "\n",
    "# only keep the valid file record\n",
    "valid_data = data.iloc[valid_files].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da577b26-ccfb-4cc9-8b0f-ea9a6517ddf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 135587\n",
      "Validation set size: 33897\n",
      "Test set size: 42372\n"
     ]
    }
   ],
   "source": [
    "# add a weights return of the dataset class\n",
    "class ScenicDataset(Dataset):\n",
    "    def __init__(self, data_frame, root_dir, transform=None):\n",
    "        self.data_frame = data_frame\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        # calculate the weight of each score\n",
    "        # self.class_weights = 1. / data_frame['average'].value_counts()\n",
    "        # self.data_frame['weight'] = self.data_frame['average'].map(self.class_weights)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.data_frame.iloc[idx]['filename']) # combine root address with filename\n",
    "        try:\n",
    "            image = Image.open(img_name).convert('RGB')\n",
    "        except (IOError, SyntaxError) as e:\n",
    "          # return None, None, None\n",
    "            return self.__getitem__((idx + 1) % len(self)) # return the next sample\n",
    "        rating = self.data_frame.iloc[idx]['average']\n",
    "        # weight = self.data_frame.iloc[idx]['weight']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(rating, dtype=torch.float32) #, torch.tensor(weight, dtype=torch.float32)\n",
    "\n",
    "# centered crop\n",
    "data_transforms = transforms.Compose([\n",
    "    # transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)\n",
    "])\n",
    "\n",
    "image_dataset = ScenicDataset(data_frame=data,\n",
    "                              root_dir='/home/ubuntu/landscape-aesthetics/data/external/scenicornot/',\n",
    "                              transform=data_transforms)\n",
    "\n",
    "# seperate the training and testing sets\n",
    "train_val_size = int(0.8 * len(image_dataset))\n",
    "test_size = len(image_dataset) - train_val_size\n",
    "train_val_dataset, test_dataset = torch.utils.data.random_split(image_dataset, [train_val_size, test_size])\n",
    "\n",
    "# seperate the training and validation set from training set\n",
    "train_size = int(0.8 * len(train_val_dataset))\n",
    "val_size = len(train_val_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [train_size, val_size])\n",
    "\n",
    "# modify dataloader, return weights\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "print(f\"Training set size: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation set size: {len(val_loader.dataset)}\")\n",
    "print(f\"Test set size: {len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fa432d-0963-4295-9349-588db4eeb3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple regression model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 32 * 32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab9c79f-bb67-4b2f-93d3-8cf9c99e60e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the training code\n",
    "def train_model(scheduler, model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = torch.stack(images), torch.stack(labels)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            # weighted_loss = (loss * weights).sum()\n",
    "            # weighted_loss.backward()\n",
    "            # optimizer.step()\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}')\n",
    "        \n",
    "        # evaluate model\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = torch.stack(images), torch.stack(labels)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f'Validation Loss: {val_loss}')\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "model = SimpleCNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4) # add a regularization term\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)  # Learning rate scheduler\n",
    "\n",
    "# training model\n",
    "train_losses, val_losses = train_model(scheduler, model, criterion, optimizer, train_loader, val_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe935b-305e-4c00-954c-b6220b8ac3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training curve\n",
    "def plot_training_curves(train_losses, val_losses):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.plot(epochs, train_losses, 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, val_losses, 'ro-', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_curves(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d62f5a-d8cb-4f91-bc78-bfce9542b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    val_labels = []\n",
    "    val_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = torch.stack(images), torch.stack(labels)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    mae = mean_absolute_error(val_labels, val_predictions)\n",
    "    r2 = r2_score(val_labels, val_predictions)\n",
    "    print(f'Test Loss: {test_loss}')\n",
    "    print(f'Validation MAE: {mae}')\n",
    "    print(f'Validation RÂ²: {r2}')\n",
    "    \n",
    "    return test_loss, mae, r2\n",
    "\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a25334-926f-4884-b8cc-7e4d283d41cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), '/home/ubuntu/landscape-aesthetics/models/simple_cnn_1.pth')\n",
    "print(\"Model saved to simple_cnn_1.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
