{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fbf7557-5258-4c8e-a224-96dc66f601a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aed9420-92b3-44d6-9de2-689417da2c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# Download the required files.\n",
    "\n",
    "def download_file(url, save_path):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful.\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded {save_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fad19667-454d-4968-bb65-d57faba109ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "def load_model_for_binary_classification(model_name='googlenet', model_dir='../../data/external/Places365/model'):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    if model_name == 'googlenet':\n",
    "        model_file = os.path.join(model_dir, 'googlenet_places365.pth')\n",
    "        model_url = 'http://places2.csail.mit.edu/models_places365/googlenet_places365.pth'\n",
    "    elif model_name == 'alexnet':\n",
    "        model_file = os.path.join(model_dir, 'alexnet_places365.pth')\n",
    "        model_url = 'http://places2.csail.mit.edu/models_places365/alexnet_places365.pth'\n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_name} not supported.\")\n",
    "\n",
    "    if not os.path.exists(model_file):\n",
    "        print(f\"{model_file} does not exist. Downloading...\")\n",
    "        download_file(model_url, model_file)\n",
    "\n",
    "    if model_name == 'googlenet':\n",
    "        model = models.googlenet(num_classes=365)\n",
    "    elif model_name == 'alexnet':\n",
    "        model = models.alexnet(num_classes=365)\n",
    "\n",
    "    checkpoint = torch.load(model_file, map_location=lambda storage, loc: storage, weights_only=True)\n",
    "    state_dict = {str.replace(k, 'module.', ''): v for k, v in checkpoint['state_dict'].items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    if model_name == 'alexnet':\n",
    "        model.classifier[-1] = torch.nn.Linear(model.classifier[-1].in_features, 2)\n",
    "    elif model_name == 'googlenet':\n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
    "\n",
    "    print(f\"Model {model_name} for binary classification loaded successfully.\")\n",
    "    return model\n",
    "\n",
    "# model = load_model_for_binary_classification(model_name='vgg16')\n",
    "# model = load_model_for_binary_classification(model_name='googlenet')\n",
    "# model = load_model_for_binary_classification(model_name='alexnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2462bab8-0190-4b31-bdc7-3a576e88d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandscapesDataset(Dataset):\n",
    "    def __init__(self, data_frame, root_dir, transform=None):\n",
    "        self.data_frame = data_frame\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.label = self.data_frame['landscape']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.root_dir, self.data_frame.iloc[idx, 0]) \n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')  \n",
    "        except (IOError, SyntaxError) as e:\n",
    "            print(f\"Error opening image at {image_path}, skipping to next.\")\n",
    "            return self.__getitem__((idx + 1) % len(self))  \n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        label = int(self.data_frame.iloc[idx, 1])\n",
    "        return image, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71459913-7eb9-41f1-a181-4e8201e80f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 2040\n",
      "Validation set size: 510\n"
     ]
    }
   ],
   "source": [
    "data_path = '/home/ubuntu/landscape-aesthetics/data/processed/landscape_handmade/landscapes.csv'\n",
    "image_folder = Path('/home/ubuntu/landscape-aesthetics')\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "        # transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)\n",
    "    ])\n",
    "\n",
    "image_dataset = LandscapesDataset(data_frame=data,\n",
    "                              root_dir=image_folder,\n",
    "                              transform=data_transforms)\n",
    "\n",
    "train_size = int(0.8 * len(image_dataset))\n",
    "val_size = len(image_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(image_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "print(f\"Training set size: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation set size: {len(val_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94f9e262-52b2-4094-af52-c82166984abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model alexnet for binary classification loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model = load_model_for_binary_classification(model_name='alexnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3125c0f-305d-4a88-b7f0-c4ca454eccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "def train_places365_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=10):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        total_batches = len(train_loader)\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=total_batches, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_idx, (images, labels) in progress_bar:\n",
    "            images, labels = torch.stack(images), torch.stack(labels)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = torch.stack(images), torch.stack(labels)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        scheduler.step()\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "    return train_losses, val_losses, model\n",
    "\n",
    "train_losses, val_losses, model = train_places365_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=10)\n",
    "\n",
    "torch.save(model.state_dict(), 'places365_binary_model_alexnet.pth')\n",
    "\n",
    "def plot_training_curves(train_losses, val_losses):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.plot(epochs, train_losses, 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, val_losses, 'ro-', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_curves(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1539a764-c81e-47f2-8ce6-30a683f2d08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1.0, Count: 1566, Ratio: 61.41%\n",
      "Label: 0.0, Count: 984, Ratio: 38.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|████████████████████| 2550/2550 [29:35<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.33%\n",
      "Recall: 78.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(weights=None)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model.load_state_dict(torch.load('../../models/places365_binary_model.pth', map_location='cpu', weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "data_path = '/home/ubuntu/landscape-aesthetics/data/processed/landscape_handmade/landscapes.csv'\n",
    "image_folder = Path('/home/ubuntu/landscape-aesthetics')\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "        # transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)\n",
    "    ])\n",
    "\n",
    "\n",
    "# sampled_data = data.sample(n=100, random_state=123) \n",
    "image_paths = data.iloc[:, 0].tolist()\n",
    "true_labels = data.iloc[:, 1].tolist()\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(true_labels)\n",
    "\n",
    "total_samples = len(true_labels)\n",
    "label_ratios = {label: count / total_samples for label, count in label_counts.items()}\n",
    "\n",
    "for label, ratio in label_ratios.items():\n",
    "    print(f\"Label: {label}, Count: {label_counts[label]}, Ratio: {ratio * 100:.2f}%\")\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for img_path in tqdm(image_paths, desc=\"Processing images\"):\n",
    "\n",
    "    image_path = os.path.join(image_folder, img_path) \n",
    "    \n",
    "    img = data_transforms(Image.open(image_path).convert('RGB')) .unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(img)\n",
    "    \n",
    "    _, predicted = torch.max(output, 1)\n",
    "    predictions.append(predicted.item())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "recall = recall_score(true_labels, predictions, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'Recall: {recall * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d718f4-d603-4e49-ac61-ca39c07c778d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
